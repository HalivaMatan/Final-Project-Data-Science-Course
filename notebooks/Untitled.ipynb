{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comic-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from cluter_faces import ClusterFaces\n",
    "from face_detection import FaceDetection\n",
    "from action_recognition import ActionRecognition\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceDetection = FaceDetection()\n",
    "\n",
    "vs = cv2.VideoCapture(\"../dataset/videos/video4.mp4\")\n",
    "faces = []\n",
    "\n",
    "#Loop Video Stream\n",
    "while True:\n",
    "\n",
    "    (grabbed, frame) = vs.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    temp = faceDetection.detect_faces(frame, \"video4.mp4\")\n",
    "    faces.extend(temp)\n",
    "    \n",
    "#     frame = resizeImage(frame)\n",
    "    cv2.imshow('camera2',frame) \n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-drain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faces = pd.DataFrame(faces)\n",
    "face_encodings = faces['face_encoding'].tolist()\n",
    "clusterFaces = ClusterFaces()\n",
    "pca_df_all_data, pca_df = clusterFaces.cluster_faces(faces, face_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biological-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-27 20:43:59,085] [TfPoseEstimator] [INFO] loading graph from C:\\Users\\Haliva\\Data Science\\Data Science Course - Final Project\\libs\\action_recognition\\models\\graph/mobilenet_thin/graph_opt.pb(default size=656x368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Haliva\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_human_detection import DetectorAPI\n",
    "actionRecognition = ActionRecognition()\n",
    "faceDetection = FaceDetection()\n",
    "# initialize person detector\n",
    "model_path = 'frozen_inference_graph.pb'\n",
    "odapi = DetectorAPI(path_to_ckpt=model_path)\n",
    "threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convertible-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDetectedItems(classId, threshold):\n",
    "    filter_arr = []\n",
    "    boxes, scores, classes, num = odapi.processFrame(frame)\n",
    "    \n",
    "    print(\"threshold\", threshold)\n",
    "    print(\"scores\", scores)\n",
    "    print(\"classId\", classId)\n",
    "    print(\"classes\", classes)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        if classes[i] == classId and scores[i] > threshold:\n",
    "            filter_arr.append(boxes[i])\n",
    "    \n",
    "    return filter_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ideal-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "def getPersonId(face_encodings, unknown_face_encoding):\n",
    "    \n",
    "    face_distances = face_recognition.face_distance(face_encodings, unknown_face_encoding)\n",
    "    best_match_index = np.argmin(face_distances)\n",
    "    print(best_match_index)\n",
    "    best_match = pca_df_all_data.iloc[best_match_index]\n",
    "    print(\"best_match name:  {}\\n\".format(best_match[\"image_path\"]))\n",
    "    print(\"cluster:  {}\\n\".format(best_match[\"cluster\"]))\n",
    "    print(\"distance:  {}\".format(face_distances[best_match_index]))\n",
    "    \n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-reconstruction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 11.417446851730347\n",
      "Elapsed Time: 1.0740952491760254\n",
      "threshold 0.85\n",
      "scores [0.9772127270698547, 0.9547072649002075, 0.8729246854782104, 0.8107669949531555, 0.7650820016860962, 0.7317894697189331, 0.6249147057533264, 0.3211374878883362, 0.3170890510082245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "classId 1\n",
      "classes [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "filter_arr [(781, 2008, 2160, 2695), (53, 1265, 1911, 2798), (405, 2001, 1944, 2804)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "vs = cv2.VideoCapture(\"../dataset/videos/video4.mp4\")\n",
    "\n",
    "counter = 0;\n",
    "rec = [];\n",
    "\n",
    "#Loop Video Stream\n",
    "while True:\n",
    "\n",
    "    (grabbed, frame) = vs.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "        \n",
    "    boxes, scores, classes, num = odapi.processFrame(frame)\n",
    "    \n",
    "    # Class 1 represents human\n",
    "    filter_arr = filterDetectedItems(1, threshold)\n",
    "    \n",
    "    print(\"filter_arr\", filter_arr)\n",
    "    \n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    # Visualization of the results of a detection.\n",
    "    for box in filter_arr:\n",
    "        \n",
    "            y, x, y2, x2 = [ v for v in box]\n",
    "            \n",
    "            cv2.rectangle(frame,(x,y),(x2,y2),(255,0,0),2)\n",
    "\n",
    "            human = frame[y:y2, x:x2]\n",
    "            \n",
    "            small_frame = cv2.resize(human, (0, 0),  fx=0.3, fy=0.3)\n",
    "            \n",
    "#             cv2.imshow('human2',small_frame)\n",
    "\n",
    "            faces = faceDetection.detect_faces(human, \"0.mp4\")\n",
    "            \n",
    "            skeletons, humans, scale_h = actionRecognition.detect_skeletons(human)\n",
    "            \n",
    "            print(\"faces\")\n",
    "            print(len(faces))\n",
    "\n",
    "            print(\"skeletons\")\n",
    "            print(len(skeletons))\n",
    "            \n",
    "#             personId = getPersonId(face_encodings, face[\"face_encoding\"])\n",
    "            \n",
    "#             skeleton = skeletons[0]\n",
    "#             label = actionRecognition.recognize_action(skeleton, personId)\n",
    "#             print(\"---------------------label---------------\", label)\n",
    "\n",
    "#             today = date.today()\n",
    "#             print(\"Today's date:\", today)\n",
    "\n",
    "#             rec.append({\"time\": today,\"cluster\":  best_match[\"cluster\"], \"action\": label})\n",
    "            \n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-library",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
